{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import gym\nimport matplotlib as plt\nimport tensorflow as tf\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport time","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class simpleLayer(tf.keras.layers.Layer):\n    def __init__(self,dense_units=7,last_layer=False):\n        super(simpleLayer, self).__init__()\n        self.dense_units = dense_units\n        self.batchnorm = tf.keras.layers.BatchNormalization(axis= -1) # channell first\n        self.last = last_layer\n        if not self.last:\n            self.dropout = tf.keras.layers.Dropout(0.3)\n    \n    def build(self,input_shape):\n        self.dense = tf.keras.layers.Dense(self.dense_units,input_shape = input_shape)\n        \n        \n    def call(self, input):\n        out = self.dense(input)\n        #out = self.batchnorm(out)\n        if not self.last:\n            out = self.dropout(out)\n        return out","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\ndef shuffle(list1, list2):\n    temp = zip(list1, list2)\n    np.random.shuffle(temp)\n    a, b = zip(*temp)\n    return a ,b","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class actorNet(tf.keras.Model):\n    def __init__(self):\n        super(actorNet, self).__init__()\n        self.layer1 = simpleLayer()\n        self.layer2 = simpleLayer()\n        self.layer3 = simpleLayer(last_layer=True)\n        self.dense = tf.keras.layers.Dense(2, activation = self.myActivation)\n    \n    def call(self, input):\n        x = self.layer1(input)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.dense(x)\n        return x\n    \n    def myActivation(self,x):\n        return tf.keras.activations.softmax(x,axis=-1)\n    \n    \n    def ppoLoss(self, y_true, y_pred, advantage, old_prediction):\n        ENTROPY_LOSS = 5e-3\n        LOSS_CLIPPING = 0.2\n        prob = tf.keras.backend.sum(y_true * y_pred, axis=-1)\n        prob = tf.dtypes.cast(prob, tf.float32)\n        old_prob = tf.keras.backend.sum(y_true * old_prediction, axis=-1)\n        old_prob = tf.dtypes.cast(old_prob, tf.float32)\n        r = prob/(old_prob + 1e-10)\n        return -tf.keras.backend.mean(tf.keras.backend.minimum(r * advantage, tf.keras.backend.clip(r, min_value=1 - LOSS_CLIPPING, max_value=1 + LOSS_CLIPPING) * advantage) + ENTROPY_LOSS * -(prob * tf.keras.backend.log(prob + 1e-10)))\n        \n    \n    \n    def learn(self, observe, action, advantage, old_prediction, epochs, batch, shuffle):\n        self.optimaizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n        for epoch in range(epochs):\n            batch_observe = observe[batch*epoch:batch*(epoch+1)]\n            batch_action = action[batch*epoch:batch*(epoch+1)]\n            if shuffle is True:\n                batch_observe, batch_action = shuffle(batch_observe, batch_action)\n            print(\"Start of epoch %d\" % (epoch,))\n            for step, (obs_batch_train, act_batch_train) in enumerate(zip(batch_observe,batch_action)):\n                with tf.GradientTape() as tape:\n                    print(obs_batch_train.shape)\n                    pred_action = self.call(obs_batch_train)\n                    loss = self.ppoLoss(act_batch_train,pred_action,advantage, old_prediction )\n                    \n\n                print(\"the model loss is :%d\" % loss)\n                grads = tape.gradient(loss, self.trainable_weights)\n                self.optimaizer.apply_gradients(zip(grads, self.trainable_weights))\n","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class criticNet(tf.keras.Model):\n    def __init__(self):\n        super(criticNet, self).__init__()\n        self.layer1 = simpleLayer()\n        self.layer2 = simpleLayer()\n        self.layer3 = simpleLayer(last_layer=True)\n        self.dense = tf.keras.layers.Dense(1, activation = self.myActivation)\n    def call(self, input):\n        x = self.layer1(input)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.dense(x)\n        return x\n    def myActivation(self,x):\n        return tf.keras.activations.linear(x)\n    \n    def learn(self, observe, reward, epochs, batch ,shuffle):\n        \n        self.optimaizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n        for i in range(epochs):\n            batch_observe = observe[batch*i:batch*(i+1)]\n            batch_reward = reward[batch*i:batch*(i+1)]\n            if shuffle is True:\n                batch_observe, batch_reward = shuffle(batch_observe, batch_reward)\n            for j, (batch_observe_train, batch_reward_train) in enumerate(zip(batch_observe,batch_reward)):\n                with tf.GradientTape() as tape:\n                    print(batch_observe_train.shape)\n                    pred_reward = self.call(batch_observe_train)\n                    loss = tf.keras.losses.MSE(batch_reward_train, pred_reward)\n                print(\"the model loss is :%d\" % loss)\n                grads = tape.gradient(loss, self.trainable_weights)\n                self.optimaizer.apply_gradients(zip(grads, self.trainable_weights))\n                ","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Gamerecord():\n    def __init__(self):\n        self.observations = []\n        self.actions = []\n        self.actionPredictions = []\n        self.reward_steps = []\n        self.reward_total = []\n        self.first_epoch_index = 0\n    \n    def add_reward(self, reward,done, gamma=0.9):\n        self.reward_steps.append(reward)\n        self.reward_total.append(reward)\n        list_size = len(self.reward_total)\n        if done is True:\n            self.first_epoch_index = list_size - 1\n        power = 1\n        for i in range(list_size-2, self.first_epoch_index-1, -1):\n            self.reward_total[i] += (gamma**power)*reward\n            power += 1\n            \n    def reset(self):\n        self.observations = []\n        self.actions = []\n        self.actionPredictions = []\n        self.reward_steps = []\n        self.reward_total = []\n            ","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BUFFER_SIZE = 256\nEPISODES = 10\nEPOCHS = 10\nBATCH_SIZE = 256\n\nclass Agent():\n    def __init__(self):\n        self.actionNet = actorNet()\n        self.criticalNet = criticNet()\n        self.env = gym.make('CartPole-v0')\n        self.episode = 0\n        self.observation = self.env.reset()\n        self.records = Gamerecord()\n        self.gradient_steps = 0\n\n        \n    def reset_env(self):\n        self.episode += 1\n        self.observation = self.env.reset()\n        \n    def choose_action(self):\n        p = self.actionNet.predict(self.observation.reshape(1, 4))\n        action = np.random.choice(2, p=np.nan_to_num(p[0]))\n        action_matrix = np.zeros(2)\n        action_matrix[action] = 1\n        return action, action_matrix, p   \n    \n    \n    \n    \n    \n    def get_experiance(self):\n        \n        while len(self.records.observations) < BUFFER_SIZE:\n          \n            print(\"processing :\", len(self.records.observations),\"\\n\")\n            action, action_matrix, predicted_action = self.choose_action()\n            observation, reward, done, info = self.env.step(action)\n            \n            self.records.observations.append(self.observation)\n            self.records.actions.append(action_matrix)\n            self.records.actionPredictions.append(predicted_action)\n            self.records.add_reward(reward, done)\n            \n            self.observation = observation\n\n            if done:\n                self.reset_env()\n\n        obs, action, pred, reward = np.array(self.records.observations).reshape(-1,1,4), np.array(self.records.actions), np.array(self.records.actionPredictions).reshape(-1,1,2), np.array(self.records.reward_total).reshape(-1,1,1)\n        return obs, action, pred, reward\n    \n    def learn(self):\n        \n        while self.episode < EPISODES:\n            obs, action, pred, reward = self.get_experiance()\n            obs, action, pred, reward = obs[:BUFFER_SIZE], action[:BUFFER_SIZE], pred[:BUFFER_SIZE], reward[:BUFFER_SIZE]\n            old_prediction = pred\n            \n            pred_values = self.criticalNet.predict(obs)\n            advantage = reward - pred_values\n            \n            self.actionNet.learn(obs, action, advantage, old_prediction, EPOCHS, BATCH_SIZE, shuffle=False)\n            self.criticalNet.learn(obs, reward, EPOCHS, BATCH_SIZE, shuffle=False)\n\n            self.gradient_steps += 1\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if __name__ == '__main__':\n    ag = Agent()\n    ag.learn()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}